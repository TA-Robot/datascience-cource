# 1.1.5 代表的な分布

確率論において、様々な確率分布が存在します。これらの分布は、現実世界の不確実な現象をモデル化するための数学的道具であり、自然科学から社会科学、工学から金融まであらゆる分野で応用されています。本章では、基本的な確率分布とその性質、そして分布間の関係性について詳しく解説します。各分布の背後にある直感的理解と実世界での意味を大切にしながら、数学的な厳密性も追求していきましょう。

## 1. 離散確率分布

離散確率分布とは、確率変数が取りうる値が離散的（数えられる）である確率分布です。日常生活の多くの現象—サイコロの目、当選数、故障回数など—は離散的な値として観測されます。

### 1.1 ベルヌーイ分布 (Bernoulli Distribution)

コイン投げのような、「成功」か「失敗」の2つの結果しかない単一の試行を表す最も基本的な分布です。シンプルながらも確率論の基礎を形作る重要な分布であり、より複雑な分布のビルディングブロックとなります。

- **例**：コイン投げ、合否判定、Yes/No質問の回答、スイッチのON/OFF状態
- **パラメータ**：成功確率 $ p $ （$ 0 \leq p \leq 1 $）
- **確率質量関数**：$ P(X = x) = p^x (1-p)^{1-x} $ （$ x \in \{0, 1\} $）
- **期待値**：$ E[X] = p $
- **分散**：$ V[X] = p(1-p) $
- **モーメント母関数**：$ M_X(t) = (1-p) + pe^t $
- **特性関数**：$ \phi_X(t) = (1-p) + pe^{it} $

**直感的理解**：成功確率 $ p $ のコイン投げを1回行い、表が出れば1、裏が出れば0という値を取る確率変数です。分散 $ p(1-p) $ は $ p=0.5 $ のとき最大となり、このとき最も「不確実性」が高いことを示しています。$ p $ が0または1に近づくほど、結果はより予測可能になります。

**応用例**：
- 選挙での投票行動（特定の候補に投票するか否か）
- 商品購入の意思決定（買うか買わないか）
- 疾病の有無（陽性か陰性か）
- 品質管理における合格/不合格判定
- 機械学習におけるバイナリ分類問題

**歴史的背景**：
ジェイコブ・ベルヌーイ（1654-1705）にちなんで名付けられました。彼はスイスの数学者で確率論の基礎を築いた先駆者の一人です。彼の著書「推測術（Ars Conjectandi）」は、死後1713年に出版され、確率論における初の体系的な著作として知られています。

**数学的特徴**：
ベルヌーイ分布は最も単純な離散確率分布ですが、高次モーメントやキュムラントの計算が容易であり、より複雑な分布（二項分布やポアソン分布など）を導出する基礎となります。$ p=0.5 $ のとき、この分布は対称になりますが、それ以外では非対称です。

### 1.2 二項分布 (Binomial Distribution)

同じ成功確率を持つ独立な試行を複数回繰り返したときの成功回数を表す分布です。ベルヌーイ試行の自然な拡張であり、多くの実践的な状況でのカウントデータのモデル化に使用されます。

- **例**：10回のコイン投げで表が出る回数、100人の患者のうち治療に反応する人数、製品バッチから抽出した50個のサンプルのうち欠陥のある数
- **パラメータ**：試行回数 $ n $、成功確率 $ p $
- **確率質量関数**：$ P(X = k) = \binom{n}{k} p^k (1-p)^{n-k} $ （$ k = 0, 1, 2, \ldots, n $）
- **期待値**：$ E[X] = np $
- **分散**：$ V[X] = np(1-p) $
- **モーメント母関数**：$ M_X(t) = (1-p+pe^t)^n $
- **特性関数**：$ \phi_X(t) = (1-p+pe^{it})^n $

二項係数は「n個から異なるk個を選ぶ組み合わせの数」を表します：

```math
\binom{n}{k} = \frac{n!}{k!(n-k)!}
```

**直感的理解**：成功確率 $ p $ の試行を $ n $ 回繰り返すとき、成功する回数を表します。例えば、歩行者10人のうち何人が赤信号を無視して横断するかなど。

**計算例**：
公正なコイン（$ p = 0.5 $）を5回投げて、表が丁度3回出る確率は：
$ P(X = 3) = \binom{5}{3} (0.5)^3 (0.5)^2 = 10 \times 0.125 \times 0.25 = 0.3125 $

**近似**：
$ n $ が大きく $ p $ が0.5に近いとき、二項分布は正規分布 $ N(np, np(1-p)) $ で近似できます。

**応用例**：
- 品質管理：製品ロットからランダムに抽出したサンプルで不良品の個数の予測
- 臨床試験：治療効果の分析
- マーケティング：キャンペーンの反応率推定
- 選挙予測：投票行動の分析と予測
- リスク評価：保険数理における損害頻度のモデル化

**関連する分布**：
- $ n=1 $ のとき、二項分布はベルヌーイ分布に還元されます
- $ n \to \infty $, $ p \to 0 $ かつ $ np = \lambda $ （一定）のとき、二項分布はポアソン分布 $ Poisson(\lambda) $ に近づきます
- $ p $ が非常に小さいとき、二項分布はポアソン分布で近似できます

### 1.3 幾何分布 (Geometric Distribution)

最初の成功が現れるまでの試行回数を表す分布です。

- **例**：サイコロを振って初めて6が出るまでの回数、初めて当たりくじを引くまでの回数
- **パラメータ**：成功確率 $ p $
- **確率質量関数**：$ P(X = k) = (1-p)^{k-1}p $ （$ k = 1, 2, 3, \ldots $）
- **期待値**：$ E[X] = \frac{1}{p} $
- **分散**：$ V[X] = \frac{1-p}{p^2} $
- **モーメント母関数**：$ M_X(t) = \frac{pe^t}{1-(1-p)e^t} $ （$ t < -\ln(1-p) $）
- **特性関数**：$ \phi_X(t) = \frac{pe^{it}}{1-(1-p)e^{it}} $

**直感的理解**：成功するまで何回試行が必要か、という「待ち時間」を表します。例えば、サイコロで「6」が出るまで何回振る必要があるかなど。

**無記憶性**：
幾何分布は「無記憶性」という重要な性質を持ちます。すでに $ m $ 回失敗した後で、さらに $ k $ 回目で初めて成功する条件付き確率は、最初から $ k $ 回目で成功する確率と同じです：

$ P(X = m+k | X > m) = P(X = k) $

**例題**：
成功確率が $ p = 0.2 $ のとき、初めて成功するまでに平均何回の試行が必要ですか？
$ E[X] = \frac{1}{p} = \frac{1}{0.2} = 5 $ 回

**応用例**：
- 保険（初めての事故が起こるまでの期間）
- ギャンブル（初めて大当たりが出るまでの試行回数）
- 通信（データパケットが正常に送信されるまでの再送回数）

### 1.4 負の二項分布 (Negative Binomial Distribution)

$ r $ 回目の成功が現れるまでの総試行回数を表す分布で、幾何分布の一般化です。

- **例**：5人目の顧客が来店するまでに断る見込み客の数
- **パラメータ**：成功回数 $ r > 0 $、成功確率 $ p $
- **確率質量関数**：$ P(X = k) = \binom{k-1}{r-1} p^r (1-p)^{k-r} $ （$ k = r, r+1, r+2, \ldots $）
- **期待値**：$ E[X] = \frac{r}{p} $
- **分散**：$ V[X] = \frac{r(1-p)}{p^2} $

**直感的理解**：$ r $ 回成功するまでに必要な総試行回数を表します。幾何分布は $ r = 1 $ の特殊な場合です。

**応用例**：
- 疫学（特定数の感染者が現れるまでの接触回数）
- マーケティング（目標販売数達成までの営業訪問回数）
- スポーツ統計（特定得点に達するまでの試合数）

### 1.5 ポアソン分布 (Poisson Distribution)

一定時間や空間内で発生する事象の回数を表す分布です。

- **例**：1時間あたりに来店する客数、単位面積あたりの微粒子の数、1ページあたりの誤字数
- **パラメータ**：平均発生率 $ \lambda > 0 $
- **確率質量関数**：$ P(X = k) = \frac{\lambda^k e^{-\lambda}}{k!} $ （$ k = 0, 1, 2, \ldots $）
- **期待値**：$ E[X] = \lambda $
- **分散**：$ V[X] = \lambda $
- **モーメント母関数**：$ M_X(t) = e^{\lambda(e^t-1)} $
- **特性関数**：$ \phi_X(t) = e^{\lambda(e^{it}-1)} $

**直感的理解**：めったに起こらないが可能性がたくさんある事象（例：交通事故、停電）の発生回数を表します。

**ポアソン過程**：
事象がランダムに発生する過程で、以下の条件を満たすものをポアソン過程と呼びます：
1. 異なる時間間隔での事象の発生は互いに独立
2. 短い時間間隔 $ \Delta t $ での事象発生確率は $ \lambda \Delta t $ に比例
3. 同時に2つ以上の事象が発生する確率は無視できる

**二項分布との関係**：
$ n $ が大きく $ p $ が小さいとき、二項分布 $ B(n,p) $ はポアソン分布 $ Poisson(np) $ で近似できます：

$ \binom{n}{k} p^k (1-p)^{n-k} \approx \frac{(np)^k e^{-np}}{k!} $ （$ n \to \infty $, $ p \to 0 $, $ np = \lambda $ 一定）

**加法性**：
独立なポアソン分布 $ Poisson(\lambda_1) $ と $ Poisson(\lambda_2) $ に従う確率変数の和は、$ Poisson(\lambda_1 + \lambda_2) $ に従います。

**応用例**：
- 通信（単位時間あたりの通話着信数）
- 放射線物理学（単位時間あたりの放射性崩壊数）
- 金融（保険金請求の発生回数）
- Webサーバー（単位時間あたりのアクセス数）

### 1.6 超幾何分布 (Hypergeometric Distribution)

有限の母集団からの非復元抽出における成功回数を表す分布です。

- **例**：52枚のトランプから5枚引いたときのハートの枚数、投票箱から無作為抽出した票の中の賛成票数
- **パラメータ**：母集団サイズ $ N $、母集団中の成功要素数 $ K $、標本サイズ $ n $
- **確率質量関数**：$ P(X = k) = \frac{\binom{K}{k}\binom{N-K}{n-k}}{\binom{N}{n}} $ （$ \max(0, n+K-N) \leq k \leq \min(n, K) $）
- **期待値**：$ E[X] = n\frac{K}{N} $
- **分散**：$ V[X] = n\frac{K}{N}(1-\frac{K}{N})(\frac{N-n}{N-1}) $

**直感的理解**：くじ引きなど、一度引いたものを戻さない（非復元）場合の当たりの数を表します。例えば、100人の応募者（うち40人が女性）から10人を選ぶとき、女性が何人含まれるかなど。

**二項分布との違い**：
二項分布は「復元抽出」（または試行が独立）の場合、超幾何分布は「非復元抽出」の場合に使います。母集団のサイズが標本サイズに比べて十分大きい場合（$ N \gg n $）、超幾何分布は二項分布 $ B(n, \frac{K}{N}) $ で近似できます。

**分散の補正係数**：
二項分布と比較すると、超幾何分布の分散には $ \frac{N-n}{N-1} $ という補正係数が掛かります。これは「有限母集団補正」と呼ばれます。

**応用例**：
- 投票箱の検査（開票前の予測）
- 質量検査（バッチからのサンプル検査）
- カードゲーム（特定のカードを引く確率）
- 監査（不正検出のためのサンプリング）

### 1.7 多項分布 (Multinomial Distribution)

二項分布を3つ以上のカテゴリに拡張した分布です。

- **例**：サイコロを10回振ったときの、各目が出る回数
- **パラメータ**：試行回数 $ n $、各カテゴリの確率 $ p_1, p_2, \ldots, p_k $ （$ \sum_{i=1}^{k} p_i = 1 $）
- **確率質量関数**：$ P(X_1 = x_1, \ldots, X_k = x_k) = \frac{n!}{x_1! \cdots x_k!} p_1^{x_1} \cdots p_k^{x_k} $ （$ \sum_{i=1}^{k} x_i = n $）
- **期待値**：$ E[X_i] = np_i $
- **分散**：$ V[X_i] = np_i(1-p_i) $
- **共分散**：$ Cov(X_i, X_j) = -np_ip_j $ （$ i \neq j $）

**直感的理解**：複数のカテゴリがある場合の出現回数を同時に考える分布です。例えば、アンケートで「賛成」「反対」「どちらとも言えない」の3つの回答がそれぞれ何回あるかなど。

**応用例**：
- 言語処理（テキスト中の単語出現頻度）
- マーケティング（複数商品の選択パターン）
- 政治学（複数政党の得票数）
- 遺伝学（多遺伝子の分離）

## 2. 連続確率分布

連続確率分布とは、確率変数が連続的な値（実数など）を取る確率分布です。

### 2.1 一様分布 (Uniform Distribution)

指定された区間内のすべての値が等しい確率で発生する分布です。

- **例**：0から1までのランダムな実数を生成、ルーレットの結果
- **パラメータ**：下限 $ a $、上限 $ b $ （$ a < b $）
- **確率密度関数**：$ f(x) = \frac{1}{b-a} $ （$ a \leq x \leq b $）
- **累積分布関数**：$ F(x) = \frac{x-a}{b-a} $ （$ a \leq x \leq b $）
- **期待値**：$ E[X] = \frac{a+b}{2} $
- **分散**：$ V[X] = \frac{(b-a)^2}{12} $
- **モーメント母関数**：$ M_X(t) = \frac{e^{tb}-e^{ta}}{t(b-a)} $ （$ t \neq 0 $）
- **特性関数**：$ \phi_X(t) = \frac{e^{itb}-e^{ita}}{it(b-a)} $ （$ t \neq 0 $）

**直感的理解**：サイコロの目のように、すべての値が同じ確率で出る分布です。例えば、ランダムに生成された数字が0.3と0.7の間にある確率は、0.1と0.5の間にある確率と同じです。

**エントロピー**：
連続確率分布の中で、一様分布は指定された範囲で最大のエントロピーを持ちます。つまり、最も「ランダム」または「予測不可能」な分布です。

**乱数生成**：
コンピュータのほとんどの乱数生成器は、まず区間 $ [0,1] $ の一様分布に従う値を生成し、それを変換して他の分布に従う乱数を作ります。

**応用例**：
- シミュレーション（初期条件のランダム化）
- 暗号化（鍵の生成）
- ゲーム（公平な結果の生成）
- 量子力学（測定前の状態）

### 2.2 正規分布（ガウス分布）(Normal/Gaussian Distribution)

自然界で最も頻繁に観察される連続分布で、釣鐘型の対称な形をしています。

- **例**：身長、体重、テストの点数、測定誤差
- **パラメータ**：平均 $ \mu $、標準偏差 $ \sigma > 0 $
- **確率密度関数**：

```math
f(x) = \frac{1}{\sigma\sqrt{2\pi}} e^{-\frac{(x-\mu)^2}{2\sigma^2}} \quad (-\infty < x < \infty)
```

- **累積分布関数**：解析的な形では表せないが、標準正規分布では $ \Phi(x) $ と表記
- **期待値**：$ E[X] = \mu $
- **分散**：$ V[X] = \sigma^2 $
- **モーメント母関数**：$ M_X(t) = e^{\mu t + \frac{\sigma^2 t^2}{2}} $
- **特性関数**：$ \phi_X(t) = e^{i\mu t - \frac{\sigma^2 t^2}{2}} $

**標準正規分布**：
$ \mu = 0 $、$ \sigma = 1 $ の場合を標準正規分布と呼び、$ Z \sim N(0,1) $ と表記します。任意の正規分布 $ X \sim N(\mu, \sigma^2) $ は、$ Z = \frac{X-\mu}{\sigma} $ という変換で標準正規分布に変換できます。

**確率計算**：
標準正規分布では、以下のような特徴があります：
- $ P(-1 < Z < 1) \approx 0.6827 $ （1シグマ範囲）
- $ P(-2 < Z < 2) \approx 0.9545 $ （2シグマ範囲）
- $ P(-3 < Z < 3) \approx 0.9973 $ （3シグマ範囲）

**中心極限定理との関係**：
多くの独立した確率変数の和は、それらの分布に関わらず、適切に正規化すると正規分布に近づきます。

**多変量正規分布**：
複数の確率変数の同時分布として、正規分布は多変量正規分布に拡張できます：

```math
f(\mathbf{x}) = \frac{1}{(2\pi)^{n/2}|\Sigma|^{1/2}} e^{-\frac{1}{2}(\mathbf{x}-\boldsymbol{\mu})^T \Sigma^{-1} (\mathbf{x}-\boldsymbol{\mu})}
```

ここで $ \mathbf{x} $ と $ \boldsymbol{\mu} $ は $ n $ 次元ベクトル、$ \Sigma $ は $ n \times n $ の共分散行列です。

**正規分布の加法性**：
独立な正規分布 $ X \sim N(\mu_X, \sigma_X^2) $ と $ Y \sim N(\mu_Y, \sigma_Y^2) $ に従う確率変数の和は、$ X + Y \sim N(\mu_X + \mu_Y, \sigma_X^2 + \sigma_Y^2) $ に従います。

**応用例**：
- 統計的検定（t検定、z検定など）
- 信号処理（ノイズのモデル化）
- 金融（資産価格の変動）
- 品質管理（製造誤差）
- 心理測定（知能指数、性格特性）

### 2.3 指数分布 (Exponential Distribution)

事象間の待ち時間を表す分布です。

- **例**：次の顧客が来店するまでの時間、機械の故障までの時間、放射性崩壊までの時間
- **パラメータ**：率パラメータ $ \lambda > 0 $ （しばしば尺度パラメータ $ \beta = 1/\lambda $ も使用）
- **確率密度関数**：$ f(x) = \lambda e^{-\lambda x} $ （$ x \geq 0 $）
- **累積分布関数**：$ F(x) = 1 - e^{-\lambda x} $ （$ x \geq 0 $）
- **期待値**：$ E[X] = \frac{1}{\lambda} $
- **分散**：$ V[X] = \frac{1}{\lambda^2} $
- **モーメント母関数**：$ M_X(t) = \frac{\lambda}{\lambda-t} $ （$ t < \lambda $）
- **特性関数**：$ \phi_X(t) = \frac{\lambda}{\lambda-it} $

**直感的理解**：「無記憶性」を持つ分布で、すでにどれだけ待ったかに関わらず、これから先の待ち時間の分布は同じです。例えば、バスを10分待っても、これから先のバスが来るまでの時間の分布は、バス停に着いたばかりの人と同じです。

**無記憶性の数学的表現**：
$ P(X > s + t | X > s) = P(X > t) $ for all $ s, t \geq 0 $

**ポアソン過程との関係**：
ポアソン過程において、連続する事象間の時間間隔は指数分布に従います。率パラメータ $ \lambda $ のポアソン過程の事象間隔は、率パラメータ $ \lambda $ の指数分布に従います。

**最小値の分布**：
独立な指数分布 $ Exp(\lambda_i) $ に従う確率変数 $ X_i $ の最小値 $ \min(X_1, X_2, \ldots, X_n) $ は、$ Exp(\sum_{i=1}^{n} \lambda_i) $ に従います。

**応用例**：
- 信頼性工学（機器の寿命）
- 待ち行列理論（サービス時間）
- 生存分析（生存時間）
- 放射性崩壊（半減期）

### 2.4 ワイブル分布 (Weibull Distribution)

指数分布を一般化した分布で、故障率が時間とともに変化する場合のモデルとして使用されます。

- **例**：製品の寿命、風速のモデル化
- **パラメータ**：形状パラメータ $ k > 0 $、尺度パラメータ $ \lambda > 0 $
- **確率密度関数**：$ f(x) = \frac{k}{\lambda} (\frac{x}{\lambda})^{k-1} e^{-(x/\lambda)^k} $ （$ x \geq 0 $）
- **累積分布関数**：$ F(x) = 1 - e^{-(x/\lambda)^k} $ （$ x \geq 0 $）
- **期待値**：$ E[X] = \lambda \Gamma(1 + \frac{1}{k}) $
- **分散**：$ V[X] = \lambda^2 [\Gamma(1 + \frac{2}{k}) - \Gamma^2(1 + \frac{1}{k})] $

**直感的理解**：製品の故障率が時間とともに変化する場合に適しています。$ k < 1 $ のとき故障率は時間とともに減少（初期不良）、$ k = 1 $ のとき一定（指数分布と一致）、$ k > 1 $ のとき増加（経年劣化）します。

**特殊な場合**：
- $ k = 1 $ のとき、指数分布
- $ k = 2 $ のとき、レイリー分布
- $ k = 3.5 $ のとき、正規分布に近似

**応用例**：
- 信頼性工学（機械部品の寿命）
- 風力発電（風速の統計的特性）
- 材料科学（強度分析）
- 保険（生存分析）

### 2.5 ガンマ分布 (Gamma Distribution)

指数分布の一般化で、複数の事象が発生するまでの時間を表します。

- **例**：k回目の事象が発生するまでの時間、降雨量
- **パラメータ**：形状パラメータ $ k > 0 $、尺度パラメータ $ \theta > 0 $ （または率パラメータ $ \beta = 1/\theta $）
- **確率密度関数**：

```math
f(x) = \frac{x^{k-1}e^{-x/\theta}}{\theta^k \Gamma(k)} \quad (x > 0)
```

ここで $ \Gamma(k) $ はガンマ関数です。

- **期待値**：$ E[X] = k\theta $
- **分散**：$ V[X] = k\theta^2 $
- **モーメント母関数**：$ M_X(t) = (1-\theta t)^{-k} $ （$ t < 1/\theta $）
- **特性関数**：$ \phi_X(t) = (1-i\theta t)^{-k} $

**ガンマ関数**：
$ \Gamma(k) = \int_0^{\infty} x^{k-1} e^{-x} dx $ で定義される関数で、自然数 $ n $ に対しては $ \Gamma(n) = (n-1)! $ となります。

**直感的理解**：ポアソン過程でk回目の事象が発生するまでの時間を表します。k=1のとき指数分布になります。

**加法性**：
独立なガンマ分布 $ Gamma(k_1, \theta) $ と $ Gamma(k_2, \theta) $ に従う確率変数の和は、$ Gamma(k_1 + k_2, \theta) $ に従います（尺度パラメータが同じ場合）。

**特殊な場合**：
- $ k = 1 $ のとき、指数分布
- $ k = n/2 $、$ \theta = 2 $ のとき、自由度 $ n $ のカイ二乗分布

**応用例**：
- 待ち行列理論（複数事象までの待ち時間）
- 気象学（降水量のモデル化）
- ベイズ統計（共役事前分布）
- 金融（収益率の分布）

### 2.6 ベータ分布 (Beta Distribution)

0と1の間の値をとる確率変数を表す分布です。確率や比率のモデル化に適しています。

- **例**：ある事象の成功確率の事前分布、投票率の分布
- **パラメータ**：形状パラメータ $ \alpha > 0 $、$ \beta > 0 $
- **確率密度関数**：

```math
f(x) = \frac{x^{\alpha-1}(1-x)^{\beta-1}}{B(\alpha,\beta)} \quad (0 \leq x \leq 1)
```

ここで $ B(\alpha,\beta) $ はベータ関数です：

```math
B(\alpha,\beta) = \frac{\Gamma(\alpha)\Gamma(\beta)}{\Gamma(\alpha+\beta)}
```

- **期待値**：$ E[X] = \frac{\alpha}{\alpha+\beta} $
- **分散**：$ V[X] = \frac{\alpha\beta}{(\alpha+\beta)^2(\alpha+\beta+1)} $
- **モーメント母関数**：解析的な形では一般に表せない
- **特性関数**：解析的な形では一般に表せない

**直感的理解**：確率や割合など0〜1の範囲の値をモデル化するのに適しています。パラメータ $ \alpha $ と $ \beta $ を変えることで、様々な形状（U字型、一様型、山型など）を表現できます。

**ベイズ統計での役割**：
ベータ分布は二項分布の共役事前分布として重要な役割を果たします。事前分布が $ Beta(\alpha, \beta) $ で、二項分布のデータ（n回中k回成功）を観測した後、事後分布は $ Beta(\alpha+k, \beta+n-k) $ になります。この性質により、ベイズ統計での逐次学習に便利です。

**特殊な場合**：
- $ \alpha = \beta = 1 $ のとき、区間 $ [0,1] $ の一様分布
- $ \alpha = \beta $ のとき、$ x = 0.5 $ に関して対称な分布
- $ \alpha, \beta < 1 $ のとき、U字型の分布
- $ \alpha, \beta > 1 $ のとき、単峰性の分布
- $ \alpha = 1, \beta = 1 $ のとき、一様分布

**ベータ関数の計算**：
ベータ関数はガンマ関数を使って計算できます。例えば、$ \alpha = 2, \beta = 3 $ の場合：
$ B(2,3) = \frac{\Gamma(2)\Gamma(3)}{\Gamma(5)} = \frac{1! \times 2!}{4!} = \frac{1 \times 2}{24} = \frac{1}{12} $

**応用例**：
- ベイズ統計（事前分布）
- 信頼性分析（タスク完了率）
- マーケティング（コンバージョン率の分布）
- 選挙予測（投票率のモデリング）
- A/Bテスト（成功率の推定）

### 2.7 コーシー分布 (Cauchy Distribution)

裾が非常に重い分布で、期待値や分散が存在しないという特徴を持ちます。

- **例**：共鳴現象、ローレンツ曲線、外れ値を多く含むデータ
- **パラメータ**：位置パラメータ $ x_0 $、尺度パラメータ $ \gamma > 0 $
- **確率密度関数**：

```math
f(x) = \frac{1}{\pi\gamma[1+(\frac{x-x_0}{\gamma})^2]} \quad (-\infty < x < \infty)
```

- **累積分布関数**：$ F(x) = \frac{1}{2} + \frac{1}{\pi}\arctan(\frac{x-x_0}{\gamma}) $
- **期待値**：存在しない
- **分散**：存在しない
- **特性関数**：$ \phi_X(t) = e^{ix_0t-\gamma|t|} $

**直感的理解**：正規分布より極端な値が出やすい分布で、統計的な性質が通常の分布と大きく異なります。例えば、コーシー分布からのサンプルの平均値は、サンプルサイズを増やしても収束しません。

**期待値が存在しない理由**：
コーシー分布は裾が非常に重いため、積分 $ \int_{-\infty}^{\infty} x f(x) dx $ が収束せず、期待値が定義できません。これは、極端な値（外れ値）が出る確率が、正規分布などと比べて非常に高いためです。

**安定分布としての性質**：
コーシー分布は安定分布の一種で、独立なコーシー分布に従う確率変数の線形結合も同じ型のコーシー分布に従います。つまり、$ X_1, X_2, \ldots, X_n $ が独立で同一のコーシー分布に従うとき、その平均 $ \frac{1}{n}\sum_{i=1}^{n} X_i $ も同じコーシー分布に従います。

**生成方法**：
標準正規分布に従う2つの独立な確率変数 $ X $ と $ Y $ があるとき、その比 $ Z = \frac{X}{Y} $ は標準コーシー分布（$ x_0 = 0, \gamma = 1 $）に従います。

**物理学での応用**：
コーシー分布はローレンツ分布とも呼ばれ、共鳴現象や散乱過程など物理現象のモデル化に使われます。

**応用例**：
- 物理学（共鳴現象）
- 分光学（スペクトル線の形状）
- ロバスト統計（耐外れ値性の評価）
- 金融（極端な価格変動のモデル化）
- 信号処理（干渉パターン）

### 2.8 対数正規分布 (Log-normal Distribution)

正の値を取り、対数をとると正規分布に従う確率変数の分布です。

- **例**：資産価格、所得分布、反応時間、生物の体長
- **パラメータ**：対数平均 $ \mu $、対数標準偏差 $ \sigma > 0 $
- **確率密度関数**：

```math
f(x) = \frac{1}{x\sigma\sqrt{2\pi}} e^{-\frac{(\ln x-\mu)^2}{2\sigma^2}} \quad (x > 0)
```

- **累積分布関数**：$ F(x) = \Phi(\frac{\ln x - \mu}{\sigma}) $ （$ x > 0 $）
- **期待値**：$ E[X] = e^{\mu + \sigma^2/2} $
- **分散**：$ V[X] = (e^{\sigma^2}-1)e^{2\mu+\sigma^2} $
- **モーメント母関数**：存在しない
- **モーメント**：$ E[X^k] = e^{k\mu + k^2\sigma^2/2} $
- **特性関数**：解析的な形では一般に表せない

**直感的理解**：多くの独立した要因が乗法的に作用するプロセスの結果としてよく現れます。例えば、経済活動や生物学的プロセスなど、相対的な変化（パーセンテージ）が重要な場合に適しています。

**グラフの特徴**：
非対称で、正の値にのみ分布し、右側に長い裾を持ちます。パラメータによっては、左に偏った分布にもなり得ます。

**生成方法**：
$ Z \sim N(0,1) $ が標準正規分布に従うとき、$ X = e^{\mu + \sigma Z} $ は対数正規分布 $ LN(\mu, \sigma^2) $ に従います。

**正規分布との関係**：
$ X $ が対数正規分布 $ LN(\mu, \sigma^2) $ に従うとき、$ Y = \ln X $ は正規分布 $ N(\mu, \sigma^2) $ に従います。

**中央値と最頻値**：
- 中央値：$ e^\mu $
- 最頻値：$ e^{\mu-\sigma^2} $

**応用例**：
- 経済学（所得分布、資産価格）
- 生物学（生物体のサイズ、細胞の大きさ）
- 金融（株価変動）
- 材料科学（粒子サイズの分布）
- 反応時間（心理学実験）

## 3. 確率分布の特性関数と応用

### 3.1 特性関数 (Characteristic Function)

特性関数は確率分布を完全に特徴づける関数で、分布の性質や確率変数の和の分布を調べるのに役立ちます。

**定義**：
確率変数 $ X $ の特性関数 $ \phi_X(t) $ は次のように定義されます：

```math
\phi_X(t) = E[e^{itX}] = \int_{-\infty}^{\infty} e^{itx}f_X(x)dx \quad (t \in \mathbb{R})
```

ここで $ i $ は虚数単位（$ i^2 = -1 $）、$ f_X(x) $ は $ X $ の確率密度関数（または確率質量関数）です。

**主な性質**：
1. $ |\phi_X(t)| \leq 1 $ かつ $ \phi_X(0) = 1 $
2. 特性関数は一意に確率分布を決定する（異なる分布は異なる特性関数を持つ）
3. 独立な確率変数の和の特性関数は、各変数の特性関数の積になる：$ \phi_{X+Y}(t) = \phi_X(t) \cdot \phi_Y(t) $
4. 線形変換の特性関数：$ \phi_{aX+b}(t) = e^{itb} \phi_X(at) $

**直感的理解**：
特性関数はフーリエ変換に似た働きをし、分布の「指紋」のような役割を果たします。すべての確率分布は一意の特性関数を持ち、逆に特性関数から分布を復元できます。

**モーメントとの関係**：
特性関数の導関数から分布のモーメントを求められます：

```math
E[X^n] = \frac{1}{i^n} \frac{d^n}{dt^n}\phi_X(t)|_{t=0}
```

**主な分布の特性関数**：
- 標準正規分布：$ \phi_X(t) = e^{-t^2/2} $
- ポアソン分布（パラメータ $ \lambda $）：$ \phi_X(t) = e^{\lambda(e^{it}-1)} $
- 指数分布（パラメータ $ \lambda $）：$ \phi_X(t) = \frac{\lambda}{\lambda-it} $

### 3.2 モーメント母関数 (Moment Generating Function, MGF)

モーメント母関数は分布のモーメント（平均、分散など）を簡単に求めるために使用されます。

**定義**：
確率変数 $ X $ のMGF $ M_X(t) $ は次のように定義されます：

```math
M_X(t) = E[e^{tX}] = \int_{-\infty}^{\infty} e^{tx}f_X(x)dx
```

ただし、この積分が収束する $ t $ の範囲でのみ定義されます。

**MGFと特性関数の比較**：
1. MGFは実数 $ t $ に対して定義されるが、特性関数は虚数 $ it $ を使用
2. MGFは常に存在するとは限らないが、特性関数は常に存在する
3. MGFからモーメントを簡単に求められる：$ E[X^n] = \frac{d^n}{dt^n}M_X(t)|_{t=0} $
4. 両方とも独立確率変数の和に対して乗法性を持つ

**MGFが存在しない例**：
コーシー分布やt分布など、裾が重い分布はMGFが存在しないことがあります。このような場合でも特性関数は常に存在します。

**モーメントの計算例**：
正規分布 $ N(\mu, \sigma^2) $ のMGFは $ M_X(t) = e^{\mu t + \sigma^2 t^2/2} $ です。
これを $ t $ で微分すると：$ M'_X(t) = (\mu + \sigma^2 t)e^{\mu t + \sigma^2 t^2/2} $
$ t = 0 $ を代入すると：$ M'_X(0) = \mu = E[X] $

**MGFの応用**：
- 確率分布の識別
- 確率変数の和の分布の導出
- 中心極限定理の証明
- 平均、分散、歪度、尖度などのモーメントの計算

### 3.3 レヴィの連続性定理 (Lévy's Continuity Theorem)

レヴィの連続性定理とは、確率変数列の分布収束と特性関数の関係を示す重要な定理です。


**定理の内容**：
確率変数列 $ \{X_n\} $ の特性関数 $ \phi_{X_n}(t) $ が、すべての $ t $ について $ \phi(t) $ に収束するならば、$ \phi(t) $ がある確率変数 $ X $ の特性関数であるとき、$ X_n $ は分布収束において $ X $ に収束する。

つまり：
$ \lim_{n \to \infty} \phi_{X_n}(t) = \phi_X(t) $ for all $ t \in \mathbb{R} $ ならば $ X_n \stackrel{d}{\to} X $

**直感的理解**：
特性関数の収束は、確率分布自体の収束を保証します。これは極限を考える上で非常に便利なツールです。特性関数は扱いやすいことが多いため、複雑な分布の収束を証明するときに役立ちます。

**逆も成立**：
分布収束 $ X_n \stackrel{d}{\to} X $ ならば、特性関数も収束します：$ \lim_{n \to \infty} \phi_{X_n}(t) = \phi_X(t) $ for all $ t \in \mathbb{R} $

**応用例**：
- 中心極限定理の証明
- 確率変数の和の極限分布の導出
- 無限分解可能分布の特徴付け
- 安定分布の研究

### 3.4 中心極限定理への応用

中心極限定理は、特性関数を使って証明できる重要な定理です。

**中心極限定理の概要**：
独立同一分布に従う確率変数 $ X_1, X_2, \ldots, X_n $ があり、$ E[X_i] = \mu $、$ V[X_i] = \sigma^2 $ とするとき、

```math
\frac{\sum_{i=1}^{n} X_i - n\mu}{\sigma\sqrt{n}} \xrightarrow{d} N(0,1)
```

つまり、標準化された和の分布は、サンプルサイズが大きくなるにつれて標準正規分布に収束します。


**特性関数を使った証明の概略**：
1. $ Z_n = \frac{\sum_{i=1}^{n} X_i - n\mu}{\sigma\sqrt{n}} $ の特性関数を考える
2. テイラー展開を使って、$ n \to \infty $ のとき $ \phi_{Z_n}(t) \to e^{-t^2/2} $ を示す
3. $ e^{-t^2/2} $ は標準正規分布の特性関数なので、レヴィの連続性定理により $ Z_n $ は標準正規分布に分布収束する

**近似精度**：
中心極限定理による近似の精度は、元の分布の形状や標本サイズに依存します。一般に：
- サンプルサイズが大きいほど近似は良くなる
- 元の分布が対称的であるほど、少ないサンプルサイズでも良い近似が得られる
- 非対称性が強いほど、良い近似を得るためにより大きなサンプルサイズが必要

**ベリー=エッセーン定理**：
中心極限定理による近似の誤差の上限を与えます。確率変数の3次モーメント $ \rho = E[|X-\mu|^3] $ が有限のとき、近似誤差は $ O(1/\sqrt{n}) $ のオーダーです。

**応用例**：
- 統計的検定（z検定など）
- 信頼区間の構築
- モンテカルロ法（乱数和のモデル化）
- リスク評価（多数の小さなリスクの合計）

### 3.5 分布間の距離

異なる確率分布間の「距離」や「類似度」を測る方法がいくつかあります。

#### 3.5.1 KLダイバージェンス (Kullback-Leibler Divergence)

2つの確率分布 $ P $ と $ Q $ の違いを測る非対称な指標です。

**離散確率分布の場合**：

```math
D_{KL}(P||Q) = \sum_{x} P(x) \log\frac{P(x)}{Q(x)}
```

**連続確率分布の場合**：

```math
D_{KL}(P||Q) = \int_{-\infty}^{\infty} p(x) \log\frac{p(x)}{q(x)} dx
```

**性質**：
1. $ D_{KL}(P||Q) \geq 0 $ かつ $ D_{KL}(P||Q) = 0 $ iff $ P = Q $
2. 非対称性：一般に $ D_{KL}(P||Q) \neq D_{KL}(Q||P) $
3. 三角不等式を満たさない（距離の公理を満たさない）

**直感的理解**：
分布 $ P $ を分布 $ Q $ で近似したときに失われる情報量を測ります。情報理論で重要な役割を果たします。

**計算例**：
2つの正規分布 $ P = N(\mu_1, \sigma_1^2) $ と $ Q = N(\mu_2, \sigma_2^2) $ の間のKLダイバージェンス：

```math
D_{KL}(P||Q) = \log\frac{\sigma_2}{\sigma_1} + \frac{\sigma_1^2 + (\mu_1-\mu_2)^2}{2\sigma_2^2} - \frac{1}{2}
```

**応用例**：
- 機械学習（変分推論、生成モデル）
- 情報理論（データ圧縮）
- 統計的モデル選択
- 強化学習（方策最適化）

#### 3.5.2 ワッサースタイン距離 (Wasserstein Distance)

2つの確率分布間の「輸送コスト」を表す距離です。

**1-ワッサースタイン距離（$ W_1 $）**：
1次元の場合、次のように表されます：

```math
W_1(P, Q) = \int_{0}^{1} |F_P^{-1}(u) - F_Q^{-1}(u)| du
```

ここで $ F_P^{-1} $ と $ F_Q^{-1} $ はそれぞれ分布 $ P $ と $ Q $ の逆累積分布関数です。

**最適輸送問題**：
ワッサースタイン距離は、ある確率質量から別の確率質量への最小コストの「輸送計画」を見つける問題として解釈できます。

**性質**：
1. $ W_p(P, Q) \geq 0 $ かつ $ W_p(P, Q) = 0 $ iff $ P = Q $
2. 対称性：$ W_p(P, Q) = W_p(Q, P) $
3. 三角不等式：$ W_p(P, R) \leq W_p(P, Q) + W_p(Q, R) $
4. KLダイバージェンスとは異なり、サポートが重なっていない分布間でも有限の値を取る

**直感的理解**：
ある分布から別の分布に「土を移動させる」最小コストを表します。例えば、砂の山を別の形に変形するのに必要な労力をイメージしてください。

**計算例**：
2つの正規分布 $ P = N(\mu_1, \sigma_1^2) $ と $ Q = N(\mu_2, \sigma_2^2) $ の間の2-ワッサースタイン距離：

```math
W_2(P, Q) = \sqrt{(\mu_1 - \mu_2)^2 + (\sigma_1 - \sigma_2)^2}
```

**応用例**：
- 画像処理（色移送）
- 生成モデル（GAN、VAEの訓練）
- 分布間の補間
- クラスタリング

## 4. 分布間の関係

確率分布の間には多くの興味深い関係があります。これらの関係を理解することで、複雑な確率モデルをより単純な基本分布で近似できるようになります。

### 4.1 加法性と畳み込み

独立な確率変数の和の分布は、元の分布の畳み込みによって得られます。いくつかの分布では、この性質が特に簡単な形で表されます。

**正規分布の加法性**：
独立な正規分布 $ X \sim N(\mu_X, \sigma_X^2) $ と $ Y \sim N(\mu_Y, \sigma_Y^2) $ に従う確率変数の和は、$ X + Y \sim N(\mu_X + \mu_Y, \sigma_X^2 + \sigma_Y^2) $ に従います。

**例題**：
身長が $ N(170, 5^2) $ cm、体重が $ N(65, 3^2) $ kgの分布に従うとき、BMI $= \frac{\text{体重}}{\text{身長}^2} \times 10000$ の分布を近似的に求めてください。

**解答**：
$ X = $ 身長、$ Y = $ 体重とします。BMIは非線形関数ですが、テイラー展開を用いて一次近似すると、BMIも近似的に正規分布に従い、その平均と分散を計算できます。

**ポアソン分布の加法性**：
独立なポアソン分布 $ X \sim Poisson(\lambda_X) $ と $ Y \sim Poisson(\lambda_Y) $ に従う確率変数の和は、$ X + Y \sim Poisson(\lambda_X + \lambda_Y) $ に従います。

**例題**：
あるWebサイトでは1時間あたりのエラーが平均2回のポアソン分布に従います。別のWebサイトでは1時間あたりのエラーが平均3回のポアソン分布に従います。両方のWebサイトを同時に監視するとき、1時間あたりの総エラー数の分布は？

**解答**：
$ X + Y \sim Poisson(2 + 3) = Poisson(5) $

**ガンマ分布の加法性**：
独立なガンマ分布 $ X \sim Gamma(k_1, \theta) $ と $ Y \sim Gamma(k_2, \theta) $ に従う確率変数の和は、$ X + Y \sim Gamma(k_1 + k_2, \theta) $ に従います（尺度パラメータが同じ場合）。

### 4.2 変換と関係

**対数正規分布と正規分布**：
$ X $ が対数正規分布 $ LN(\mu, \sigma^2) $ に従うとき、$ Y = \ln X $ は正規分布 $ N(\mu, \sigma^2) $ に従います。逆に、$ Z $ が正規分布 $ N(\mu, \sigma^2) $ に従うとき、$ W = e^Z $ は対数正規分布 $ LN(\mu, \sigma^2) $ に従います。

**カイ二乗分布と正規分布**：
標準正規分布に従う独立な確率変数 $ Z_1, Z_2, \ldots, Z_n $ の二乗和 $ \sum_{i=1}^{n} Z_i^2 $ は、自由度 $ n $ のカイ二乗分布 $ \chi^2(n) $ に従います。

**F分布と正規分布**：
独立な2つのカイ二乗分布 $ U \sim \chi^2(m) $ と $ V \sim \chi^2(n) $ があるとき、$ F = \frac{U/m}{V/n} $ は自由度 $ (m, n) $ のF分布に従います。

**t分布と正規分布**：
標準正規分布 $ Z \sim N(0, 1) $ とカイ二乗分布 $ V \sim \chi^2(n) $ が独立なとき、$ T = \frac{Z}{\sqrt{V/n}} $ は自由度 $ n $ のt分布に従います。

**ベータ分布とガンマ分布**：
独立なガンマ分布 $ X \sim Gamma(\alpha, 1) $ と $ Y \sim Gamma(\beta, 1) $ があるとき、$ \frac{X}{X+Y} $ はベータ分布 $ Beta(\alpha, \beta) $ に従います。

### 4.3 近似関係

**二項分布とポアソン分布**：
$ n $ が大きく $ p $ が小さいとき、二項分布 $ B(n,p) $ はポアソン分布 $ Poisson(np) $ で近似できます。この近似は $ n \to \infty $, $ p \to 0 $, $ np = \lambda $ （一定）のときに正確になります。

**近似の精度**：
$ n = 100 $, $ p = 0.01 $ の場合、二項分布とポアソン分布（$ \lambda = 1 $）の確率質量関数の違いは0.01未満です。

**例題**：
1000人のうち、ある稀な病気にかかる確率が0.002のとき、病気にかかる人数の分布を近似してください。

**解答**：
二項分布 $ B(1000, 0.002) $ はポアソン分布 $ Poisson(1000 \times 0.002) = Poisson(2) $ で近似できます。

**二項分布と正規分布**：
$ n $ が大きいとき、二項分布 $ B(n,p) $ は正規分布 $ N(np, np(1-p)) $ で近似できます。

**近似の精度**：
この近似は $ np > 5 $ かつ $ n(1-p) > 5 $ のときに一般的に良好です。連続修正を行うと精度が向上します：$ P(X \leq k) \approx \Phi(\frac{k+0.5-np}{\sqrt{np(1-p)}}) $

**ポアソン分布と正規分布**：
$ \lambda $ が大きいとき、ポアソン分布 $ Poisson(\lambda) $ は正規分布 $ N(\lambda, \lambda) $ で近似できます。

### 4.4 極限定理

**中心極限定理**：
独立同一分布に従う確率変数 $ X_1, X_2, \ldots, X_n $ があり、$ E[X_i] = \mu $、$ V[X_i] = \sigma^2 $ とするとき、

```math
\frac{\sum_{i=1}^{n} X_i - n\mu}{\sigma\sqrt{n}} \xrightarrow{d} N(0,1)
```

この定理は、サンプルサイズが大きくなると、元の分布に関わらず標本平均が正規分布に近づくことを意味します。

**リンドベリ=レヴィ条件**：
中心極限定理が成立するための条件の一つは、3次モーメントが有限であることです。このとき、収束の速さは $ O(1/\sqrt{n}) $ のオーダーとなります。

**大数の法則**：
独立同一分布に従う確率変数の算術平均は、サンプルサイズが大きくなるにつれて期待値に収束します：

```math
\frac{1}{n}\sum_{i=1}^{n} X_i \xrightarrow{p} E[X]
```

**極値分布理論**：
独立同一分布に従う確率変数の最大値（または最小値）の極限分布は、元の分布の裾の性質によって3つのタイプ（ワイブル、グンベル、フレシェ）のいずれかになります。

### 4.5 特性関数とモーメント母関数の応用

特性関数とモーメント母関数は、確率分布の重要な数学的表現であり、分布の性質や確率変数の和の分布を調べるのに役立ちます。

#### 4.5.1 特性関数 (Characteristic Function)

**定義**：
確率変数 $ X $ の特性関数 $ \phi_X(t) $ は次のように定義されます：

```math
\phi_X(t) = E[e^{itX}] = \int_{-\infty}^{\infty} e^{itx}f_X(x)dx \quad (t \in \mathbb{R})
```

ここで $ i $ は虚数単位、$ f_X(x) $ は $ X $ の確率密度関数です。

**性質**：
1. $ |\phi_X(t)| \leq 1 $ かつ $ \phi_X(0) = 1 $
2. 特性関数は一意に確率分布を決定する
3. 独立な確率変数の和の特性関数は、各変数の特性関数の積になる：$ \phi_{X+Y}(t) = \phi_X(t) \cdot \phi_Y(t) $
4. 確率変数の線形変換の特性関数：$ \phi_{aX+b}(t) = e^{itb} \phi_X(at) $

**主な分布の特性関数**：
- 正規分布 $ N(\mu, \sigma^2) $：$ \phi_X(t) = e^{i\mu t - \sigma^2 t^2/2} $
- 指数分布 $ Exp(\lambda) $：$ \phi_X(t) = \frac{\lambda}{\lambda-it} $
- ガンマ分布 $ Gamma(k, \theta) $：$ \phi_X(t) = (1-i\theta t)^{-k} $

**例題**：
独立な確率変数 $ X_1, X_2, \ldots, X_n $ がそれぞれ $ N(\mu_i, \sigma_i^2) $ に従うとき、その和 $ S = \sum_{i=1}^{n} X_i $ の分布を求めてください。

**解答**：
特性関数を使うと、$ \phi_S(t) = \prod_{i=1}^{n} \phi_{X_i}(t) = \prod_{i=1}^{n} e^{i\mu_i t - \sigma_i^2 t^2/2} = e^{i(\sum_{i=1}^{n}\mu_i) t - (\sum_{i=1}^{n}\sigma_i^2) t^2/2} $

これは $ N(\sum_{i=1}^{n}\mu_i, \sum_{i=1}^{n}\sigma_i^2) $ の特性関数なので、$ S $ はこの正規分布に従います。

#### 4.5.2 モーメント母関数 (Moment Generating Function, MGF)

**定義**：
確率変数 $ X $ のMGF $ M_X(t) $ は次のように定義されます：

```math
M_X(t) = E[e^{tX}] = \int_{-\infty}^{\infty} e^{tx}f_X(x)dx
```

**モーメントとの関係**：
MGFの導関数からモーメントを求められます：$ E[X^n] = \frac{d^n}{dt^n}M_X(t)|_{t=0} $

**主な分布のMGF**：
- 正規分布 $ N(\mu, \sigma^2) $：$ M_X(t) = e^{\mu t + \sigma^2 t^2/2} $
- 指数分布 $ Exp(\lambda) $：$ M_X(t) = \frac{\lambda}{\lambda-t} $ （$ t < \lambda $）
- ポアソン分布 $ Poisson(\lambda) $：$ M_X(t) = e^{\lambda(e^t-1)} $

**MGFが存在しない例**：
コーシー分布やt分布など、裾が重い分布はMGFが存在しません。このような場合は特性関数を使います。

### 4.6 分布間の距離

異なる確率分布間の「距離」や「類似度」を測る方法がいくつかあります。

#### 4.6.1 KLダイバージェンス (Kullback-Leibler Divergence)

**定義**：
2つの確率分布 $ P $ と $ Q $ の間のKLダイバージェンスは次のように定義されます。

離散確率分布の場合：

```math
D_{KL}(P||Q) = \sum_{x} P(x) \log\frac{P(x)}{Q(x)}
```

連続確率分布の場合：

```math
D_{KL}(P||Q) = \int_{-\infty}^{\infty} p(x) \log\frac{p(x)}{q(x)} dx
```

**性質**：
1. $ D_{KL}(P||Q) \geq 0 $ かつ $ D_{KL}(P||Q) = 0 $ iff $ P = Q $
2. 非対称性：一般に $ D_{KL}(P||Q) \neq D_{KL}(Q||P) $
3. 三角不等式を満たさない

**例題**：
$ P = N(0, 1) $ と $ Q = N(1, 1) $ の間のKLダイバージェンスを計算してください。

**解答**：
正規分布間のKLダイバージェンスの公式を適用して、$ D_{KL}(P||Q) = \frac{(\mu_P-\mu_Q)^2}{2\sigma_Q^2} = \frac{(0-1)^2}{2 \times 1^2} = 0.5 $

**応用例**：
- 変分推論（最適近似）
- モデル選択
- 情報利得の測定
- 生成モデルの学習

#### 4.6.2 ワッサースタイン距離 (Wasserstein Distance)

**定義**：
1-ワッサースタイン距離は、1次元の場合、次のように表されます：

```math
W_1(P, Q) = \int_{0}^{1} |F_P^{-1}(u) - F_Q^{-1}(u)| du
```

ここで $ F_P^{-1} $ と $ F_Q^{-1} $ はそれぞれ分布 $ P $ と $ Q $ の逆累積分布関数です。

**性質**：
1. $ W_p(P, Q) \geq 0 $ かつ $ W_p(P, Q) = 0 $ iff $ P = Q $
2. 対称性：$ W_p(P, Q) = W_p(Q, P) $
3. 三角不等式：$ W_p(P, R) \leq W_p(P, Q) + W_p(Q, R) $
4. KLダイバージェンスとは異なり、サポートが重なっていない分布間でも有限の値を取る

**直感的理解**：
ある分布から別の分布に「土を移動させる」最小コストを表します。輸送問題として解釈できます。

**例題**：
$ P = N(0, 1) $ と $ Q = N(2, 1) $ の間の2-ワッサースタイン距離を計算してください。

**解答**：
正規分布間の2-ワッサースタイン距離の公式を適用して、$ W_2(P, Q) = \sqrt{(\mu_P - \mu_Q)^2 + (\sigma_P - \sigma_Q)^2} = \sqrt{(0 - 2)^2 + (1 - 1)^2} = 2 $

**応用例**：
- 生成モデル（GANの訓練）
- 画像処理（色移送）
- 分布間の補間
- ドメイン適応

### 4.7 レヴィの連続性定理

**定理の内容**：
確率変数列 $ \{X_n\} $ の特性関数 $ \phi_{X_n}(t) $ が、すべての $ t $ について $ \phi(t) $ に収束するならば、$ \phi(t) $ がある確率変数 $ X $ の特性関数であるとき、$ X_n $ は分布収束において $ X $ に収束します。

**分布収束**：
確率変数列 $ \{X_n\} $ が確率変数 $ X $ に分布収束するとは、すべての連続関数 $ f $ に対して、$ E[f(X_n)] \to E[f(X)] $ となることを意味します。

**実用的な意味**：
特性関数を調べることで、確率変数の極限分布を特定できます。特に、中心極限定理やその他の極限定理の証明に役立ちます。

### 4.8 実践的な分布選択

実際のデータをモデル化する際、適切な確率分布を選ぶための指針をいくつか示します。

**データの性質に基づく選択**：
- **正の値のみ**：ガンマ分布、対数正規分布、ワイブル分布
- **有界値（0〜1）**：ベータ分布
- **対称データ**：正規分布、t分布
- **カウントデータ**：ポアソン分布、負の二項分布
- **二値データ**：ベルヌーイ分布
- **重い裾を持つデータ**：t分布、コーシー分布
- **歪んだデータ**：ガンマ分布、対数正規分布

**分布選択の手順**：
1. データの性質を把握する（範囲、対称性、裾の重さなど）
2. 理論的背景から候補となる分布を考える
3. データに分布を当てはめる（最尤推定などを使用）
4. 適合度のチェック（Q-Qプロットなど）
5. 複数の候補から最適なモデルを選択（AIC、BICなどの情報量規準を使用）

**例**：
所得データが右に強く歪んでいる場合、対数正規分布やガンマ分布が適切な候補になります。対数変換したデータが正規分布に近ければ、対数正規分布が適切と判断できます。

## 5. まとめ

本章では、主要な確率分布とその性質、分布間の関係について学びました。実際のデータ分析やモデリングでは、これらの分布の特性を理解し、適切な分布を選択することが重要です。また、特性関数やモーメント母関数、分布間の距離などの概念は、より高度な確率論や統計学の基礎となります。

確率分布は、不確実性をモデル化する強力なツールであり、機械学習、統計的推論、リスク評価など様々な分野で応用されています。それぞれの分布の特性と適用場面を理解することで、より精度の高い確率モデルを構築できるようになります。
